{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Final Project | Statistical Learning\n",
    "### Author: Saniya Abushakimova\n",
    "Outline\n",
    "0. Exploratory Data Analysis\n",
    "1. Logistic Regression with Scikit-learn \n",
    "2. Reproducing the baseline result from scratch\n",
    "3. Implementing the modification "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "import random\n",
    "import time"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 0. Exploratory Data Analysis"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "data = pd.read_csv('hazelnuts_preprocessed.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>length</th>\n",
       "      <th>width</th>\n",
       "      <th>thickness</th>\n",
       "      <th>surfacearea</th>\n",
       "      <th>mass</th>\n",
       "      <th>compactness</th>\n",
       "      <th>hardness</th>\n",
       "      <th>shelltopradius</th>\n",
       "      <th>watercontent</th>\n",
       "      <th>carbohydratecontent</th>\n",
       "      <th>variety</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <td>0</td>\n",
       "      <td>11.67</td>\n",
       "      <td>12.8025</td>\n",
       "      <td>8.055075</td>\n",
       "      <td>34.65</td>\n",
       "      <td>1375.50</td>\n",
       "      <td>0.93005</td>\n",
       "      <td>19.145</td>\n",
       "      <td>4.4604</td>\n",
       "      <td>0.048668</td>\n",
       "      <td>0.175</td>\n",
       "      <td>c_avellana</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1</td>\n",
       "      <td>13.86</td>\n",
       "      <td>13.0995</td>\n",
       "      <td>7.349907</td>\n",
       "      <td>38.10</td>\n",
       "      <td>1439.55</td>\n",
       "      <td>0.93401</td>\n",
       "      <td>8.780</td>\n",
       "      <td>4.7844</td>\n",
       "      <td>0.048826</td>\n",
       "      <td>0.167</td>\n",
       "      <td>c_avellana</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2</td>\n",
       "      <td>20.53</td>\n",
       "      <td>15.5925</td>\n",
       "      <td>9.565427</td>\n",
       "      <td>49.89</td>\n",
       "      <td>1623.30</td>\n",
       "      <td>0.96217</td>\n",
       "      <td>5.120</td>\n",
       "      <td>5.2893</td>\n",
       "      <td>0.049521</td>\n",
       "      <td>0.174</td>\n",
       "      <td>c_americana</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>3</td>\n",
       "      <td>14.13</td>\n",
       "      <td>12.2220</td>\n",
       "      <td>7.182949</td>\n",
       "      <td>35.43</td>\n",
       "      <td>1412.25</td>\n",
       "      <td>0.90178</td>\n",
       "      <td>13.694</td>\n",
       "      <td>4.8168</td>\n",
       "      <td>0.049595</td>\n",
       "      <td>0.167</td>\n",
       "      <td>c_avellana</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>4</td>\n",
       "      <td>15.85</td>\n",
       "      <td>14.7240</td>\n",
       "      <td>8.622661</td>\n",
       "      <td>43.29</td>\n",
       "      <td>1512.00</td>\n",
       "      <td>0.96261</td>\n",
       "      <td>10.925</td>\n",
       "      <td>4.6296</td>\n",
       "      <td>0.050384</td>\n",
       "      <td>0.173</td>\n",
       "      <td>c_americana</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   length    width  thickness  surfacearea     mass  compactness  hardness  \\\n",
       "0   11.67  12.8025   8.055075        34.65  1375.50      0.93005    19.145   \n",
       "1   13.86  13.0995   7.349907        38.10  1439.55      0.93401     8.780   \n",
       "2   20.53  15.5925   9.565427        49.89  1623.30      0.96217     5.120   \n",
       "3   14.13  12.2220   7.182949        35.43  1412.25      0.90178    13.694   \n",
       "4   15.85  14.7240   8.622661        43.29  1512.00      0.96261    10.925   \n",
       "\n",
       "   shelltopradius  watercontent  carbohydratecontent      variety  \n",
       "0          4.4604      0.048668                0.175   c_avellana  \n",
       "1          4.7844      0.048826                0.167   c_avellana  \n",
       "2          5.2893      0.049521                0.174  c_americana  \n",
       "3          4.8168      0.049595                0.167   c_avellana  \n",
       "4          4.6296      0.050384                0.173  c_americana  "
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(201, 11)"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Sample size, the number of attributes.\n",
    "data.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Index(['length', 'width', 'thickness', 'surfacearea', 'mass', 'compactness',\n",
       "       'hardness', 'shelltopradius', 'watercontent', 'carbohydratecontent',\n",
       "       'variety'],\n",
       "      dtype='object')"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data.columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array(['c_avellana', 'c_americana', 'c_cornuta'], dtype=object)"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Number of classes.\n",
    "data[\"variety\"].unique()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "c_americana    70\n",
       "c_cornuta      66\n",
       "c_avellana     65\n",
       "Name: variety, dtype: int64"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data[\"variety\"].value_counts()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Our dataset describes hazelnuts. There are 10 different attributes, namely length, width, thickness, surfacearea, mass, compactness, hardness, shelltopradius, watercontent, carbohydratecontent. Based on these attributes, our task will be to predict variety of the hazelnut, whether it is c_avellana, c_americana or c_cornuta. Sample size is 201 hazelnuts (c_americana: 70, c_cornuta: 66, c_avellana: 65)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1. Multiclass Logistic Regression with Scikit-learn"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Normalizing the data.\n",
    "features = ['length', 'width', 'thickness', 'surfacearea', 'mass', 'compactness',\n",
    "       'hardness', 'shelltopradius', 'watercontent', 'carbohydratecontent']\n",
    "data[features]= (data[features]-data[features].mean())/data[features].std()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>length</th>\n",
       "      <th>width</th>\n",
       "      <th>thickness</th>\n",
       "      <th>surfacearea</th>\n",
       "      <th>mass</th>\n",
       "      <th>compactness</th>\n",
       "      <th>hardness</th>\n",
       "      <th>shelltopradius</th>\n",
       "      <th>watercontent</th>\n",
       "      <th>carbohydratecontent</th>\n",
       "      <th>variety</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <td>0</td>\n",
       "      <td>-1.050978</td>\n",
       "      <td>-1.121038</td>\n",
       "      <td>-0.618560</td>\n",
       "      <td>-1.149776</td>\n",
       "      <td>-1.131063</td>\n",
       "      <td>-1.138894</td>\n",
       "      <td>1.995372</td>\n",
       "      <td>-0.910924</td>\n",
       "      <td>-1.683203</td>\n",
       "      <td>1.476824</td>\n",
       "      <td>c_avellana</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1</td>\n",
       "      <td>-0.546293</td>\n",
       "      <td>-0.944773</td>\n",
       "      <td>-1.344368</td>\n",
       "      <td>-0.750403</td>\n",
       "      <td>-0.658270</td>\n",
       "      <td>-0.984142</td>\n",
       "      <td>-0.276846</td>\n",
       "      <td>-0.167581</td>\n",
       "      <td>-1.672743</td>\n",
       "      <td>-1.294160</td>\n",
       "      <td>c_avellana</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2</td>\n",
       "      <td>0.990809</td>\n",
       "      <td>0.534789</td>\n",
       "      <td>0.936000</td>\n",
       "      <td>0.614411</td>\n",
       "      <td>0.698103</td>\n",
       "      <td>0.116321</td>\n",
       "      <td>-1.079193</td>\n",
       "      <td>0.990795</td>\n",
       "      <td>-1.626835</td>\n",
       "      <td>1.130451</td>\n",
       "      <td>c_americana</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>3</td>\n",
       "      <td>-0.484071</td>\n",
       "      <td>-1.465557</td>\n",
       "      <td>-1.516213</td>\n",
       "      <td>-1.059483</td>\n",
       "      <td>-0.859788</td>\n",
       "      <td>-2.243655</td>\n",
       "      <td>0.800402</td>\n",
       "      <td>-0.093247</td>\n",
       "      <td>-1.621993</td>\n",
       "      <td>-1.294160</td>\n",
       "      <td>c_avellana</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>4</td>\n",
       "      <td>-0.087697</td>\n",
       "      <td>0.019346</td>\n",
       "      <td>-0.034360</td>\n",
       "      <td>-0.149607</td>\n",
       "      <td>-0.123471</td>\n",
       "      <td>0.133516</td>\n",
       "      <td>0.193381</td>\n",
       "      <td>-0.522733</td>\n",
       "      <td>-1.569850</td>\n",
       "      <td>0.784078</td>\n",
       "      <td>c_americana</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "     length     width  thickness  surfacearea      mass  compactness  \\\n",
       "0 -1.050978 -1.121038  -0.618560    -1.149776 -1.131063    -1.138894   \n",
       "1 -0.546293 -0.944773  -1.344368    -0.750403 -0.658270    -0.984142   \n",
       "2  0.990809  0.534789   0.936000     0.614411  0.698103     0.116321   \n",
       "3 -0.484071 -1.465557  -1.516213    -1.059483 -0.859788    -2.243655   \n",
       "4 -0.087697  0.019346  -0.034360    -0.149607 -0.123471     0.133516   \n",
       "\n",
       "   hardness  shelltopradius  watercontent  carbohydratecontent      variety  \n",
       "0  1.995372       -0.910924     -1.683203             1.476824   c_avellana  \n",
       "1 -0.276846       -0.167581     -1.672743            -1.294160   c_avellana  \n",
       "2 -1.079193        0.990795     -1.626835             1.130451  c_americana  \n",
       "3  0.800402       -0.093247     -1.621993            -1.294160   c_avellana  \n",
       "4  0.193381       -0.522733     -1.569850             0.784078  c_americana  "
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Adding artifical column of ones for a bias term.\n",
    "np_ones = np.ones([data.shape[0],1])\n",
    "data.insert(loc=0, column='bias', value=np_ones)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>bias</th>\n",
       "      <th>length</th>\n",
       "      <th>width</th>\n",
       "      <th>thickness</th>\n",
       "      <th>surfacearea</th>\n",
       "      <th>mass</th>\n",
       "      <th>compactness</th>\n",
       "      <th>hardness</th>\n",
       "      <th>shelltopradius</th>\n",
       "      <th>watercontent</th>\n",
       "      <th>carbohydratecontent</th>\n",
       "      <th>variety</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <td>0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>-1.050978</td>\n",
       "      <td>-1.121038</td>\n",
       "      <td>-0.618560</td>\n",
       "      <td>-1.149776</td>\n",
       "      <td>-1.131063</td>\n",
       "      <td>-1.138894</td>\n",
       "      <td>1.995372</td>\n",
       "      <td>-0.910924</td>\n",
       "      <td>-1.683203</td>\n",
       "      <td>1.476824</td>\n",
       "      <td>c_avellana</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1</td>\n",
       "      <td>1.0</td>\n",
       "      <td>-0.546293</td>\n",
       "      <td>-0.944773</td>\n",
       "      <td>-1.344368</td>\n",
       "      <td>-0.750403</td>\n",
       "      <td>-0.658270</td>\n",
       "      <td>-0.984142</td>\n",
       "      <td>-0.276846</td>\n",
       "      <td>-0.167581</td>\n",
       "      <td>-1.672743</td>\n",
       "      <td>-1.294160</td>\n",
       "      <td>c_avellana</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.990809</td>\n",
       "      <td>0.534789</td>\n",
       "      <td>0.936000</td>\n",
       "      <td>0.614411</td>\n",
       "      <td>0.698103</td>\n",
       "      <td>0.116321</td>\n",
       "      <td>-1.079193</td>\n",
       "      <td>0.990795</td>\n",
       "      <td>-1.626835</td>\n",
       "      <td>1.130451</td>\n",
       "      <td>c_americana</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>3</td>\n",
       "      <td>1.0</td>\n",
       "      <td>-0.484071</td>\n",
       "      <td>-1.465557</td>\n",
       "      <td>-1.516213</td>\n",
       "      <td>-1.059483</td>\n",
       "      <td>-0.859788</td>\n",
       "      <td>-2.243655</td>\n",
       "      <td>0.800402</td>\n",
       "      <td>-0.093247</td>\n",
       "      <td>-1.621993</td>\n",
       "      <td>-1.294160</td>\n",
       "      <td>c_avellana</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>4</td>\n",
       "      <td>1.0</td>\n",
       "      <td>-0.087697</td>\n",
       "      <td>0.019346</td>\n",
       "      <td>-0.034360</td>\n",
       "      <td>-0.149607</td>\n",
       "      <td>-0.123471</td>\n",
       "      <td>0.133516</td>\n",
       "      <td>0.193381</td>\n",
       "      <td>-0.522733</td>\n",
       "      <td>-1.569850</td>\n",
       "      <td>0.784078</td>\n",
       "      <td>c_americana</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   bias    length     width  thickness  surfacearea      mass  compactness  \\\n",
       "0   1.0 -1.050978 -1.121038  -0.618560    -1.149776 -1.131063    -1.138894   \n",
       "1   1.0 -0.546293 -0.944773  -1.344368    -0.750403 -0.658270    -0.984142   \n",
       "2   1.0  0.990809  0.534789   0.936000     0.614411  0.698103     0.116321   \n",
       "3   1.0 -0.484071 -1.465557  -1.516213    -1.059483 -0.859788    -2.243655   \n",
       "4   1.0 -0.087697  0.019346  -0.034360    -0.149607 -0.123471     0.133516   \n",
       "\n",
       "   hardness  shelltopradius  watercontent  carbohydratecontent      variety  \n",
       "0  1.995372       -0.910924     -1.683203             1.476824   c_avellana  \n",
       "1 -0.276846       -0.167581     -1.672743            -1.294160   c_avellana  \n",
       "2 -1.079193        0.990795     -1.626835             1.130451  c_americana  \n",
       "3  0.800402       -0.093247     -1.621993            -1.294160   c_avellana  \n",
       "4  0.193381       -0.522733     -1.569850             0.784078  c_americana  "
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Dividing the data into train and test.\n",
    "split = int(np.ceil(data.shape[0]*0.7))\n",
    "X_train, y_train = data.iloc[:split,:11], data.iloc[:split,11]\n",
    "X_test, y_test = data.iloc[split:,:11], data.iloc[split:,11]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "((141, 11), (141,), (60, 11), (60,))"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X_train.shape, y_train.shape, X_test.shape, y_test.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.linear_model import LogisticRegression\n",
    "\n",
    "# Defining a multi-class logistic regression model.\n",
    "model = LogisticRegression(multi_class='multinomial', solver='sag', max_iter=75) # SAG - Stochastic Average Gradient Descent\n",
    "\n",
    "# Fitting the model.\n",
    "model.fit(X_train, y_train)\n",
    "\n",
    "# Predicting labels for X_test.\n",
    "y_hat = model.predict(X_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "temp_2 = pd.DataFrame(columns = ['true', 'predicted'])\n",
    "temp_2['true'] = y_test\n",
    "temp_2['predicted'] = y_hat"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>true</th>\n",
       "      <th>predicted</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <td>141</td>\n",
       "      <td>c_americana</td>\n",
       "      <td>c_americana</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>142</td>\n",
       "      <td>c_avellana</td>\n",
       "      <td>c_avellana</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>143</td>\n",
       "      <td>c_cornuta</td>\n",
       "      <td>c_cornuta</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>144</td>\n",
       "      <td>c_cornuta</td>\n",
       "      <td>c_cornuta</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>145</td>\n",
       "      <td>c_avellana</td>\n",
       "      <td>c_avellana</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "            true    predicted\n",
       "141  c_americana  c_americana\n",
       "142   c_avellana   c_avellana\n",
       "143    c_cornuta    c_cornuta\n",
       "144    c_cornuta    c_cornuta\n",
       "145   c_avellana   c_avellana"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "temp_2.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.95\n"
     ]
    }
   ],
   "source": [
    "baseline_acc = model.score(X_test, y_test, sample_weight=None)\n",
    "print(baseline_acc)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2. Reproducing the baseline result from scratch"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "# One-hot encoding for our target variables.\n",
    "targets = data[\"variety\"].unique()\n",
    "\n",
    "temp_3 = np.zeros([data.shape[0],len(targets)])\n",
    "y_encoded = pd.DataFrame(temp_3)\n",
    "\n",
    "for i in range(len(targets)):\n",
    "    for j in range(len(y_encoded)):\n",
    "        if data[\"variety\"].iloc[j] == targets[i]:\n",
    "            y_encoded.iloc[j, i] = 1\n",
    "        else: \n",
    "            y_encoded.iloc[j, i] = 0\n",
    "            \n",
    "y_encoded = y_encoded.to_numpy()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array(['c_avellana', 'c_americana', 'c_cornuta'], dtype=object)"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# 'c_avellana': 1 0 0\n",
    "# 'c_americana': 0 1 0\n",
    "# 'c_cornuta': 0 0 1\n",
    "targets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0     c_avellana\n",
       "1     c_avellana\n",
       "2    c_americana\n",
       "3     c_avellana\n",
       "4    c_americana\n",
       "5      c_cornuta\n",
       "Name: variety, dtype: object"
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data['variety'].head(6)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[1., 0., 0.],\n",
       "       [1., 0., 0.],\n",
       "       [0., 1., 0.],\n",
       "       [1., 0., 0.],\n",
       "       [0., 1., 0.],\n",
       "       [0., 0., 1.]])"
      ]
     },
     "execution_count": 21,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "y_encoded[0:6]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Encoded y_train and y_test splits.\n",
    "y_encoded_train, y_encoded_test = y_encoded[:split], y_encoded[split:]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "def softmax(z, N):\n",
    "    return np.exp(z)/np.exp(z).sum(axis=1).reshape((N, 1)) \n",
    "\n",
    "def CrossEntropy(y, P):\n",
    "    return -np.vdot(y, np.log(P))\n",
    "\n",
    "def evaluate(X, y, W, N):\n",
    "    P = softmax(X @ W.T, N)\n",
    "    \n",
    "    return np.sum(CrossEntropy(y, P))\n",
    "\n",
    "def Logistic_Regression(X, y, lr, epochs):\n",
    "    start = time.time()\n",
    "    \n",
    "    N = X.shape[0]\n",
    "    K = y.shape[1]\n",
    "    d = X.shape[1] # Already contains bias term.\n",
    "    W = np.zeros([K, d]) # Initialize weights.\n",
    "    \n",
    "    z = X @ W.T # Logits.\n",
    "    P = softmax(z, N) # Probability distribution across all K classes.\n",
    "    \n",
    "    loss_list = []\n",
    "    \n",
    "    for e in range(1, epochs+1):\n",
    "        \n",
    "        # Gradient descent.\n",
    "        z = X @ W.T\n",
    "        P = softmax(z,N)\n",
    "        W = W + lr*(X.T @ (y - P)).T\n",
    "        \n",
    "        loss = evaluate(X, y, W, N) # Calculating training loss.\n",
    "        loss_list.append(loss)\n",
    "        \n",
    "        print(\"Epoch: {}, Training loss: {}\".format(e, loss))\n",
    "    \n",
    "    end = time.time()\n",
    "    \n",
    "    return W, loss_list, P, (end - start)\n",
    "\n",
    "# Making predictions.\n",
    "def predict_labels(X, W):\n",
    "    N = X.shape[0]\n",
    "    \n",
    "    predictions = []\n",
    "    z = X @ W.T # Logits.\n",
    "    P = softmax(z, N) # Probability distribution across all K classes.\n",
    "    \n",
    "    for row in P:\n",
    "        predictions.append(np.argmax(row)) # Selecting the highest probability.\n",
    "    \n",
    "    return predictions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 1, Training loss: 128.7971408361958\n",
      "Epoch: 2, Training loss: 113.27791724439095\n",
      "Epoch: 3, Training loss: 103.25985437636677\n",
      "Epoch: 4, Training loss: 96.22902531431669\n",
      "Epoch: 5, Training loss: 90.95735193776008\n",
      "Epoch: 6, Training loss: 86.80648528464488\n",
      "Epoch: 7, Training loss: 83.4191396202902\n",
      "Epoch: 8, Training loss: 80.58104742831455\n",
      "Epoch: 9, Training loss: 78.15566357818764\n",
      "Epoch: 10, Training loss: 76.05129250696658\n",
      "Epoch: 11, Training loss: 74.2035426834343\n",
      "Epoch: 12, Training loss: 72.56545547376159\n",
      "Epoch: 13, Training loss: 71.10168286122429\n",
      "Epoch: 14, Training loss: 69.78490373531284\n",
      "Epoch: 15, Training loss: 68.59353115010475\n",
      "Epoch: 16, Training loss: 67.51019312592075\n",
      "Epoch: 17, Training loss: 66.52069350027912\n",
      "Epoch: 18, Training loss: 65.61328048031635\n",
      "Epoch: 19, Training loss: 64.77811840559735\n",
      "Epoch: 20, Training loss: 64.00689746286608\n",
      "Epoch: 21, Training loss: 63.29253945118314\n",
      "Epoch: 22, Training loss: 62.62897198612406\n",
      "Epoch: 23, Training loss: 62.0109525019311\n",
      "Epoch: 24, Training loss: 61.433929179239236\n",
      "Epoch: 25, Training loss: 60.89392972170335\n",
      "Epoch: 26, Training loss: 60.38747145686598\n",
      "Epoch: 27, Training loss: 59.91148798764731\n",
      "Epoch: 28, Training loss: 59.463268845382935\n",
      "Epoch: 29, Training loss: 59.04040946700575\n",
      "Epoch: 30, Training loss: 58.64076944973388\n",
      "Epoch: 31, Training loss: 58.26243750003751\n",
      "Epoch: 32, Training loss: 57.90370183888973\n",
      "Epoch: 33, Training loss: 57.563025085797285\n",
      "Epoch: 34, Training loss: 57.23902284295826\n",
      "Epoch: 35, Training loss: 56.93044535431365\n",
      "Epoch: 36, Training loss: 56.6361617337784\n",
      "Epoch: 37, Training loss: 56.35514635087634\n",
      "Epoch: 38, Training loss: 56.086467036433426\n",
      "Epoch: 39, Training loss: 55.829274830398624\n",
      "Epoch: 40, Training loss: 55.582795041611966\n",
      "Epoch: 41, Training loss: 55.34631942795838\n",
      "Epoch: 42, Training loss: 55.119199336761184\n",
      "Epoch: 43, Training loss: 54.9008396709625\n",
      "Epoch: 44, Training loss: 54.69069356775755\n",
      "Epoch: 45, Training loss: 54.48825769379134\n",
      "Epoch: 46, Training loss: 54.29306807549388\n",
      "Epoch: 47, Training loss: 54.10469639518068\n",
      "Epoch: 48, Training loss: 53.922746693622244\n",
      "Epoch: 49, Training loss: 53.7468524282431\n",
      "Epoch: 50, Training loss: 53.576673843234545\n",
      "Epoch: 51, Training loss: 53.4118956138849\n",
      "Epoch: 52, Training loss: 53.25222473253386\n",
      "Epoch: 53, Training loss: 53.09738860789756\n",
      "Epoch: 54, Training loss: 52.94713335321144\n",
      "Epoch: 55, Training loss: 52.80122224180372\n",
      "Epoch: 56, Training loss: 52.659434311425485\n",
      "Epoch: 57, Training loss: 52.5215631009979\n",
      "Epoch: 58, Training loss: 52.387415505447585\n",
      "Epoch: 59, Training loss: 52.25681073603879\n",
      "Epoch: 60, Training loss: 52.12957937511615\n",
      "Epoch: 61, Training loss: 52.005562515477045\n",
      "Epoch: 62, Training loss: 51.884610975728876\n",
      "Epoch: 63, Training loss: 51.76658458397645\n",
      "Epoch: 64, Training loss: 51.65135152304929\n",
      "Epoch: 65, Training loss: 51.5387877312353\n",
      "Epoch: 66, Training loss: 51.42877635315058\n",
      "Epoch: 67, Training loss: 51.321207235958184\n",
      "Epoch: 68, Training loss: 51.215976466660884\n",
      "Epoch: 69, Training loss: 51.11298594664561\n",
      "Epoch: 70, Training loss: 51.012143000055765\n",
      "Epoch: 71, Training loss: 50.9133600129213\n",
      "Epoch: 72, Training loss: 50.816554100288414\n",
      "Epoch: 73, Training loss: 50.72164679886903\n",
      "Epoch: 74, Training loss: 50.62856378297608\n",
      "Epoch: 75, Training loss: 50.53723460173057\n"
     ]
    }
   ],
   "source": [
    "# Training.\n",
    "W, loss_list, p_dist, softmax_time = Logistic_Regression(X_train.to_numpy(), y_encoded_train, lr=0.001, epochs=75)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1.0\n",
      "1.0\n",
      "1.0\n",
      "0.9999999999999999\n",
      "1.0\n",
      "1.0\n",
      "1.0\n",
      "1.0\n",
      "1.0\n",
      "1.0\n",
      "1.0\n",
      "0.9999999999999999\n",
      "1.0\n",
      "0.9999999999999999\n",
      "1.0\n",
      "0.9999999999999999\n",
      "1.0\n",
      "1.0\n",
      "1.0\n",
      "1.0\n",
      "0.9999999999999998\n",
      "1.0\n",
      "1.0\n",
      "0.9999999999999999\n",
      "1.0\n",
      "1.0\n",
      "1.0\n",
      "1.0\n",
      "1.0\n",
      "1.0\n",
      "1.0\n",
      "1.0\n",
      "1.0\n",
      "0.9999999999999999\n",
      "1.0\n",
      "1.0\n",
      "1.0\n",
      "1.0\n",
      "0.9999999999999999\n",
      "0.9999999999999999\n",
      "0.9999999999999999\n",
      "1.0\n",
      "1.0\n",
      "0.9999999999999999\n",
      "1.0\n",
      "1.0\n",
      "1.0\n",
      "1.0\n",
      "0.9999999999999999\n",
      "1.0\n",
      "1.0\n",
      "1.0\n",
      "0.9999999999999999\n",
      "1.0\n",
      "0.9999999999999999\n",
      "1.0\n",
      "1.0\n",
      "1.0\n",
      "1.0\n",
      "1.0\n",
      "1.0\n",
      "1.0\n",
      "0.9999999999999999\n",
      "1.0\n",
      "0.9999999999999999\n",
      "1.0\n",
      "1.0000000000000002\n",
      "1.0\n",
      "1.0\n",
      "1.0\n",
      "0.9999999999999999\n",
      "1.0\n",
      "1.0\n",
      "1.0\n",
      "0.9999999999999999\n",
      "1.0\n",
      "1.0\n",
      "1.0\n",
      "1.0\n",
      "1.0\n",
      "1.0\n",
      "0.9999999999999999\n",
      "1.0\n",
      "1.0\n",
      "1.0\n",
      "0.9999999999999998\n",
      "1.0\n",
      "1.0\n",
      "0.9999999999999999\n",
      "0.9999999999999999\n",
      "1.0000000000000002\n",
      "0.9999999999999999\n",
      "0.9999999999999999\n",
      "1.0\n",
      "1.0\n",
      "1.0\n",
      "1.0\n",
      "1.0\n",
      "1.0\n",
      "1.0000000000000002\n",
      "1.0\n",
      "0.9999999999999999\n",
      "0.9999999999999999\n",
      "1.0\n",
      "0.9999999999999999\n",
      "1.0\n",
      "0.9999999999999999\n",
      "1.0\n",
      "1.0\n",
      "1.0\n",
      "1.0\n",
      "0.9999999999999999\n",
      "1.0\n",
      "1.0\n",
      "0.9999999999999999\n",
      "1.0\n",
      "1.0\n",
      "0.9999999999999998\n",
      "1.0\n",
      "0.9999999999999999\n",
      "1.0\n",
      "1.0000000000000002\n",
      "1.0\n",
      "1.0\n",
      "1.0\n",
      "1.0\n",
      "0.9999999999999999\n",
      "0.9999999999999999\n",
      "0.9999999999999999\n",
      "1.0\n",
      "1.0\n",
      "1.0000000000000002\n",
      "0.9999999999999999\n",
      "0.9999999999999998\n",
      "1.0\n",
      "0.9999999999999999\n",
      "1.0\n",
      "0.9999999999999999\n",
      "1.0\n",
      "1.0\n",
      "1.0\n"
     ]
    }
   ],
   "source": [
    "# Sum of probabilities across K classes are all equal to 1 (up to the nearest whole number).\n",
    "for p in p_dist:\n",
    "    print(p.sum())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "# Visualizing the results.\n",
    "plt.plot(loss_list)\n",
    "plt.xlabel(\"Epoch\")\n",
    "plt.ylabel(\"Training loss\")\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Predicting test set.\n",
    "predictions = predict_labels(X_test.to_numpy(), W)\n",
    "\n",
    "# Decoding predicted values.\n",
    "decoded_pred = []\n",
    "\n",
    "for pred in predictions:\n",
    "    if (pred==0):\n",
    "        decoded_pred.append('c_avellana')\n",
    "    if (pred==1):\n",
    "        decoded_pred.append('c_americana')\n",
    "    if (pred==2):\n",
    "        decoded_pred.append('c_cornuta')\n",
    "\n",
    "compare = pd.DataFrame()\n",
    "compare[\"predicted\"] = decoded_pred\n",
    "compare[\"true\"] = y_test.values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>predicted</th>\n",
       "      <th>true</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <td>0</td>\n",
       "      <td>c_americana</td>\n",
       "      <td>c_americana</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1</td>\n",
       "      <td>c_avellana</td>\n",
       "      <td>c_avellana</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2</td>\n",
       "      <td>c_cornuta</td>\n",
       "      <td>c_cornuta</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>3</td>\n",
       "      <td>c_cornuta</td>\n",
       "      <td>c_cornuta</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>4</td>\n",
       "      <td>c_avellana</td>\n",
       "      <td>c_avellana</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>5</td>\n",
       "      <td>c_cornuta</td>\n",
       "      <td>c_cornuta</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>6</td>\n",
       "      <td>c_avellana</td>\n",
       "      <td>c_avellana</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>7</td>\n",
       "      <td>c_cornuta</td>\n",
       "      <td>c_cornuta</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>8</td>\n",
       "      <td>c_cornuta</td>\n",
       "      <td>c_cornuta</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>9</td>\n",
       "      <td>c_avellana</td>\n",
       "      <td>c_avellana</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>10</td>\n",
       "      <td>c_cornuta</td>\n",
       "      <td>c_cornuta</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>11</td>\n",
       "      <td>c_avellana</td>\n",
       "      <td>c_avellana</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>12</td>\n",
       "      <td>c_cornuta</td>\n",
       "      <td>c_cornuta</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>13</td>\n",
       "      <td>c_cornuta</td>\n",
       "      <td>c_cornuta</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>14</td>\n",
       "      <td>c_cornuta</td>\n",
       "      <td>c_cornuta</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>15</td>\n",
       "      <td>c_cornuta</td>\n",
       "      <td>c_cornuta</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>16</td>\n",
       "      <td>c_avellana</td>\n",
       "      <td>c_avellana</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>17</td>\n",
       "      <td>c_avellana</td>\n",
       "      <td>c_avellana</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>18</td>\n",
       "      <td>c_americana</td>\n",
       "      <td>c_americana</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>19</td>\n",
       "      <td>c_avellana</td>\n",
       "      <td>c_avellana</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>20</td>\n",
       "      <td>c_avellana</td>\n",
       "      <td>c_avellana</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>21</td>\n",
       "      <td>c_avellana</td>\n",
       "      <td>c_avellana</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>22</td>\n",
       "      <td>c_americana</td>\n",
       "      <td>c_americana</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>23</td>\n",
       "      <td>c_americana</td>\n",
       "      <td>c_americana</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>24</td>\n",
       "      <td>c_cornuta</td>\n",
       "      <td>c_cornuta</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>25</td>\n",
       "      <td>c_cornuta</td>\n",
       "      <td>c_cornuta</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>26</td>\n",
       "      <td>c_cornuta</td>\n",
       "      <td>c_cornuta</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>27</td>\n",
       "      <td>c_avellana</td>\n",
       "      <td>c_avellana</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>28</td>\n",
       "      <td>c_avellana</td>\n",
       "      <td>c_avellana</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>29</td>\n",
       "      <td>c_cornuta</td>\n",
       "      <td>c_cornuta</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>30</td>\n",
       "      <td>c_avellana</td>\n",
       "      <td>c_avellana</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>31</td>\n",
       "      <td>c_americana</td>\n",
       "      <td>c_americana</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>32</td>\n",
       "      <td>c_cornuta</td>\n",
       "      <td>c_cornuta</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>33</td>\n",
       "      <td>c_cornuta</td>\n",
       "      <td>c_cornuta</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>34</td>\n",
       "      <td>c_americana</td>\n",
       "      <td>c_americana</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>35</td>\n",
       "      <td>c_americana</td>\n",
       "      <td>c_americana</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>36</td>\n",
       "      <td>c_cornuta</td>\n",
       "      <td>c_cornuta</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>37</td>\n",
       "      <td>c_avellana</td>\n",
       "      <td>c_americana</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>38</td>\n",
       "      <td>c_americana</td>\n",
       "      <td>c_americana</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>39</td>\n",
       "      <td>c_cornuta</td>\n",
       "      <td>c_cornuta</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>40</td>\n",
       "      <td>c_cornuta</td>\n",
       "      <td>c_cornuta</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>41</td>\n",
       "      <td>c_cornuta</td>\n",
       "      <td>c_cornuta</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>42</td>\n",
       "      <td>c_cornuta</td>\n",
       "      <td>c_cornuta</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>43</td>\n",
       "      <td>c_americana</td>\n",
       "      <td>c_americana</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>44</td>\n",
       "      <td>c_cornuta</td>\n",
       "      <td>c_cornuta</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>45</td>\n",
       "      <td>c_americana</td>\n",
       "      <td>c_americana</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>46</td>\n",
       "      <td>c_americana</td>\n",
       "      <td>c_americana</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>47</td>\n",
       "      <td>c_avellana</td>\n",
       "      <td>c_avellana</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>48</td>\n",
       "      <td>c_cornuta</td>\n",
       "      <td>c_cornuta</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>49</td>\n",
       "      <td>c_americana</td>\n",
       "      <td>c_americana</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>50</td>\n",
       "      <td>c_americana</td>\n",
       "      <td>c_americana</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>51</td>\n",
       "      <td>c_americana</td>\n",
       "      <td>c_americana</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>52</td>\n",
       "      <td>c_avellana</td>\n",
       "      <td>c_cornuta</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>53</td>\n",
       "      <td>c_americana</td>\n",
       "      <td>c_americana</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>54</td>\n",
       "      <td>c_avellana</td>\n",
       "      <td>c_avellana</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>55</td>\n",
       "      <td>c_avellana</td>\n",
       "      <td>c_cornuta</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>56</td>\n",
       "      <td>c_cornuta</td>\n",
       "      <td>c_cornuta</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>57</td>\n",
       "      <td>c_americana</td>\n",
       "      <td>c_americana</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>58</td>\n",
       "      <td>c_cornuta</td>\n",
       "      <td>c_cornuta</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>59</td>\n",
       "      <td>c_cornuta</td>\n",
       "      <td>c_cornuta</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "      predicted         true\n",
       "0   c_americana  c_americana\n",
       "1    c_avellana   c_avellana\n",
       "2     c_cornuta    c_cornuta\n",
       "3     c_cornuta    c_cornuta\n",
       "4    c_avellana   c_avellana\n",
       "5     c_cornuta    c_cornuta\n",
       "6    c_avellana   c_avellana\n",
       "7     c_cornuta    c_cornuta\n",
       "8     c_cornuta    c_cornuta\n",
       "9    c_avellana   c_avellana\n",
       "10    c_cornuta    c_cornuta\n",
       "11   c_avellana   c_avellana\n",
       "12    c_cornuta    c_cornuta\n",
       "13    c_cornuta    c_cornuta\n",
       "14    c_cornuta    c_cornuta\n",
       "15    c_cornuta    c_cornuta\n",
       "16   c_avellana   c_avellana\n",
       "17   c_avellana   c_avellana\n",
       "18  c_americana  c_americana\n",
       "19   c_avellana   c_avellana\n",
       "20   c_avellana   c_avellana\n",
       "21   c_avellana   c_avellana\n",
       "22  c_americana  c_americana\n",
       "23  c_americana  c_americana\n",
       "24    c_cornuta    c_cornuta\n",
       "25    c_cornuta    c_cornuta\n",
       "26    c_cornuta    c_cornuta\n",
       "27   c_avellana   c_avellana\n",
       "28   c_avellana   c_avellana\n",
       "29    c_cornuta    c_cornuta\n",
       "30   c_avellana   c_avellana\n",
       "31  c_americana  c_americana\n",
       "32    c_cornuta    c_cornuta\n",
       "33    c_cornuta    c_cornuta\n",
       "34  c_americana  c_americana\n",
       "35  c_americana  c_americana\n",
       "36    c_cornuta    c_cornuta\n",
       "37   c_avellana  c_americana\n",
       "38  c_americana  c_americana\n",
       "39    c_cornuta    c_cornuta\n",
       "40    c_cornuta    c_cornuta\n",
       "41    c_cornuta    c_cornuta\n",
       "42    c_cornuta    c_cornuta\n",
       "43  c_americana  c_americana\n",
       "44    c_cornuta    c_cornuta\n",
       "45  c_americana  c_americana\n",
       "46  c_americana  c_americana\n",
       "47   c_avellana   c_avellana\n",
       "48    c_cornuta    c_cornuta\n",
       "49  c_americana  c_americana\n",
       "50  c_americana  c_americana\n",
       "51  c_americana  c_americana\n",
       "52   c_avellana    c_cornuta\n",
       "53  c_americana  c_americana\n",
       "54   c_avellana   c_avellana\n",
       "55   c_avellana    c_cornuta\n",
       "56    c_cornuta    c_cornuta\n",
       "57  c_americana  c_americana\n",
       "58    c_cornuta    c_cornuta\n",
       "59    c_cornuta    c_cornuta"
      ]
     },
     "execution_count": 28,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "compare"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Calculating the accuracy.\n",
    "misclassified = 0\n",
    "for index, row in compare.iterrows():\n",
    "    if row[\"predicted\"]==row[\"true\"]:\n",
    "        misclassified += 1\n",
    "\n",
    "rep_acc = misclassified/compare.shape[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Baseline accuracy: 0.95\n",
      "Reproduced accuracy: 0.95\n"
     ]
    }
   ],
   "source": [
    "print(\"Baseline accuracy:\", baseline_acc)\n",
    "print(\"Reproduced accuracy:\", rep_acc)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3. Implementing the modification"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The main idea is to replace softmax function to an element-wise sigmoid."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [],
   "source": [
    "def sigmoid(z):\n",
    "    exp_z = np.exp(z)\n",
    "    return exp_z/(1+exp_z)\n",
    "\n",
    "def CrossEntropy(y, P):\n",
    "    return -np.vdot(y, np.log(P))\n",
    "\n",
    "def mod_evaluate(X, y, W):\n",
    "    P = sigmoid(X @ W.T)\n",
    "    \n",
    "    return np.sum(CrossEntropy(y, P))\n",
    "\n",
    "def mod_Logistic_Regression(X, y, lr, epochs):\n",
    "    start = time.time()\n",
    "    \n",
    "    N = X.shape[0]\n",
    "    K = y.shape[1]\n",
    "    d = X.shape[1] # Already contains bias term.\n",
    "    W = np.zeros([K, d]) # Initialize weights.\n",
    "    \n",
    "    z = X @ W.T # Logits.\n",
    "    P = sigmoid(z)\n",
    "    \n",
    "    mod_loss_list = []\n",
    "    \n",
    "    for e in range(1, epochs+1):\n",
    "        \n",
    "        # Gradient descent.\n",
    "        z = X @ W.T\n",
    "        P = sigmoid(z)\n",
    "        \n",
    "        # Change gradient here! \n",
    "        W = W + lr*(X.T @ (y - P)).T\n",
    "        \n",
    "        mod_loss = mod_evaluate(X, y, W) # Calculating training loss.\n",
    "        mod_loss_list.append(mod_loss)\n",
    "        \n",
    "        print(\"Epoch: {}, Training loss: {}\".format(e, mod_loss))\n",
    "    \n",
    "    end = time.time()\n",
    "        \n",
    "    return W, mod_loss_list, P, (end - start)\n",
    "\n",
    "# Making predictions.\n",
    "def mod_predict_labels(X, W):\n",
    "    N = X.shape[0]\n",
    "    \n",
    "    predictions = []\n",
    "    z = X @ W.T # Logits.\n",
    "    P = sigmoid(z)\n",
    "    \n",
    "    for row in P:\n",
    "        predictions.append(np.argmax(row)) # Selecting the highest value.\n",
    "    \n",
    "    return predictions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 1, Training loss: 86.13323812688976\n",
      "Epoch: 2, Training loss: 78.32173461628287\n",
      "Epoch: 3, Training loss: 72.87056287005305\n",
      "Epoch: 4, Training loss: 68.9109774668895\n",
      "Epoch: 5, Training loss: 65.92854769695263\n",
      "Epoch: 6, Training loss: 63.612530558195836\n",
      "Epoch: 7, Training loss: 61.76846764463109\n",
      "Epoch: 8, Training loss: 60.26994556293802\n",
      "Epoch: 9, Training loss: 59.031761784358416\n",
      "Epoch: 10, Training loss: 57.99458039885681\n",
      "Epoch: 11, Training loss: 57.11586106648674\n",
      "Epoch: 12, Training loss: 56.364313396363855\n",
      "Epoch: 13, Training loss: 55.71639705371671\n",
      "Epoch: 14, Training loss: 55.15404715711914\n",
      "Epoch: 15, Training loss: 54.663155867555375\n",
      "Epoch: 16, Training loss: 54.23253379451563\n",
      "Epoch: 17, Training loss: 53.85318371684983\n",
      "Epoch: 18, Training loss: 53.517782393825996\n",
      "Epoch: 19, Training loss: 53.220304019501626\n",
      "Epoch: 20, Training loss: 52.95574199878549\n",
      "Epoch: 21, Training loss: 52.71990021197544\n",
      "Epoch: 22, Training loss: 52.50923420903795\n",
      "Epoch: 23, Training loss: 52.32072883109552\n",
      "Epoch: 24, Training loss: 52.15180278485539\n",
      "Epoch: 25, Training loss: 52.00023342133632\n",
      "Epoch: 26, Training loss: 51.86409684403233\n",
      "Epoch: 27, Training loss: 51.74171977903557\n",
      "Epoch: 28, Training loss: 51.63164056451869\n",
      "Epoch: 29, Training loss: 51.53257727976437\n",
      "Epoch: 30, Training loss: 51.44340151466949\n",
      "Epoch: 31, Training loss: 51.36311663332346\n",
      "Epoch: 32, Training loss: 51.290839646749724\n",
      "Epoch: 33, Training loss: 51.22578600573258\n",
      "Epoch: 34, Training loss: 51.16725677270542\n",
      "Epoch: 35, Training loss: 51.11462774460824\n",
      "Epoch: 36, Training loss: 51.06734018549149\n",
      "Epoch: 37, Training loss: 51.024892894998914\n",
      "Epoch: 38, Training loss: 50.98683539148129\n",
      "Epoch: 39, Training loss: 50.95276202989574\n",
      "Epoch: 40, Training loss: 50.92230690744256\n",
      "Epoch: 41, Training loss: 50.89513943604124\n",
      "Epoch: 42, Training loss: 50.870960481723344\n",
      "Epoch: 43, Training loss: 50.84949898794509\n",
      "Epoch: 44, Training loss: 50.83050901355449\n",
      "Epoch: 45, Training loss: 50.81376712734827\n",
      "Epoch: 46, Training loss: 50.799070110335364\n",
      "Epoch: 47, Training loss: 50.786232924386624\n",
      "Epoch: 48, Training loss: 50.77508691220846\n",
      "Epoch: 49, Training loss: 50.765478198779576\n",
      "Epoch: 50, Training loss: 50.75726626873028\n",
      "Epoch: 51, Training loss: 50.7503226977817\n",
      "Epoch: 52, Training loss: 50.74453001942153\n",
      "Epoch: 53, Training loss: 50.7397807105755\n",
      "Epoch: 54, Training loss: 50.735976282222204\n",
      "Epoch: 55, Training loss: 50.73302646275858\n",
      "Epoch: 56, Training loss: 50.73084846350919\n",
      "Epoch: 57, Training loss: 50.729366317129134\n",
      "Epoch: 58, Training loss: 50.72851028081398\n",
      "Epoch: 59, Training loss: 50.728216297231086\n",
      "Epoch: 60, Training loss: 50.72842550695056\n",
      "Epoch: 61, Training loss: 50.72908380690014\n",
      "Epoch: 62, Training loss: 50.730141450016816\n",
      "Epoch: 63, Training loss: 50.731552681829385\n",
      "Epoch: 64, Training loss: 50.73327541019735\n",
      "Epoch: 65, Training loss: 50.73527090485864\n",
      "Epoch: 66, Training loss: 50.73750352381305\n",
      "Epoch: 67, Training loss: 50.73994046389667\n",
      "Epoch: 68, Training loss: 50.742551533190216\n",
      "Epoch: 69, Training loss: 50.74530894315786\n",
      "Epoch: 70, Training loss: 50.748187118636935\n",
      "Epoch: 71, Training loss: 50.751162523995816\n",
      "Epoch: 72, Training loss: 50.754213503952016\n",
      "Epoch: 73, Training loss: 50.75732013769742\n",
      "Epoch: 74, Training loss: 50.76046410511434\n",
      "Epoch: 75, Training loss: 50.76362856398874\n"
     ]
    }
   ],
   "source": [
    "mod_W, mod_loss_list, mod_p_dist, sigmoid_time = mod_Logistic_Regression(X_train, y_encoded_train, lr=0.001, epochs=75)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1.0264405861520771\n",
      "1.0292903346908193\n",
      "0.9992211341920321\n",
      "1.042864725975786\n",
      "0.7458552632280052\n",
      "1.1315831166999097\n",
      "1.2390957033001002\n",
      "1.0000284087043363\n",
      "0.8978603768859689\n",
      "0.9557839764354115\n",
      "1.2418903019866598\n",
      "1.107470004929312\n",
      "1.0436426264937235\n",
      "0.8011991670384114\n",
      "1.2209356133449834\n",
      "0.9158896062825789\n",
      "1.1695040475741285\n",
      "1.3200159405412881\n",
      "1.1821782126771294\n",
      "1.1715448966582147\n",
      "1.001685532570915\n",
      "1.0200071126514532\n",
      "0.9856183691420931\n",
      "1.014023964870532\n",
      "1.08986721777774\n",
      "1.0505942447757062\n",
      "1.0467937394649118\n",
      "0.7346643866409345\n",
      "1.2576600518135375\n",
      "1.036273690299632\n",
      "1.0809656076594367\n",
      "1.0219652112320974\n",
      "1.0392409322331355\n",
      "0.987170085484721\n",
      "1.223192996667136\n",
      "0.9362467088820662\n",
      "1.0496274215588148\n",
      "1.0673503602371164\n",
      "1.2325113255306714\n",
      "1.0134817090236234\n",
      "0.991429570494015\n",
      "1.213557467101324\n",
      "1.194346701224692\n",
      "0.8813813297833183\n",
      "1.068103780389453\n",
      "1.08308154375903\n",
      "1.091328780786514\n",
      "1.0057408914930168\n",
      "1.0752851729844954\n",
      "0.9533435331568599\n",
      "1.0963764971171317\n",
      "1.048353520620099\n",
      "1.129347664392901\n",
      "1.0629118664459025\n",
      "0.8428549207497343\n",
      "1.0270174689882825\n",
      "1.3651853448862266\n",
      "1.3004897536371036\n",
      "1.1726008794400309\n",
      "1.146451284565436\n",
      "1.0291554402024328\n",
      "1.0861175021432514\n",
      "0.9087493239559012\n",
      "1.052348624346406\n",
      "1.2607136910464596\n",
      "0.886369722458576\n",
      "0.9870576321503062\n",
      "1.2452393746527533\n",
      "1.1768241009475844\n",
      "1.0927792045787676\n",
      "0.9056962055495928\n",
      "0.9947118694833882\n",
      "1.0994073975100889\n",
      "1.0771273184595098\n",
      "1.1879424864169923\n",
      "1.0491976781632584\n",
      "1.0739087762555326\n",
      "1.3075104712021148\n",
      "1.1783808250672354\n",
      "1.2254061558071423\n",
      "1.217181341817619\n",
      "0.9598982253007959\n",
      "0.9801278705190613\n",
      "1.288995782105534\n",
      "1.1721333175685364\n",
      "0.9986582005309119\n",
      "1.4059895107830842\n",
      "0.9528598292448935\n",
      "1.0878049759381214\n",
      "1.1682813364571187\n",
      "1.0809322287385967\n",
      "1.1221291002794689\n",
      "1.1330334631132521\n",
      "1.0562431442865916\n",
      "1.0711677737231236\n",
      "1.2134023642676917\n",
      "0.9634070366187324\n",
      "1.1466238507485829\n",
      "1.1935292898380827\n",
      "1.1208431072718215\n",
      "1.0432720853422395\n",
      "1.1223858755972458\n",
      "1.067887006354674\n",
      "1.2276360393717072\n",
      "0.9959792024123052\n",
      "1.0614122200815153\n",
      "1.058020278542384\n",
      "1.0146457487234715\n",
      "1.0970470874772145\n",
      "1.5599066657784748\n",
      "1.4138387067871505\n",
      "1.0884141647244532\n",
      "1.0916926962593865\n",
      "1.0879379627758854\n",
      "1.3318176180210726\n",
      "1.1058101340906803\n",
      "1.4272109268052797\n",
      "1.1585212799661548\n",
      "1.290764676092739\n",
      "1.090282045022772\n",
      "1.3466079558660937\n",
      "1.1506913294226426\n",
      "1.1463411436284021\n",
      "1.2989328558361095\n",
      "1.2380546267030765\n",
      "1.1086376475962156\n",
      "1.1406272090304683\n",
      "1.0819337622039917\n",
      "1.153036604088479\n",
      "1.112617275949598\n",
      "1.1945249101963462\n",
      "1.2040150394341522\n",
      "1.2167030656514655\n",
      "1.1024314934008395\n",
      "1.3697896417939912\n",
      "1.3749333872623861\n",
      "1.1384822029289454\n",
      "1.1772382192084625\n",
      "1.1337103701195284\n",
      "1.236575107605641\n",
      "1.1836601507084665\n"
     ]
    }
   ],
   "source": [
    "# Sum of \"probabilities\" across K classes are not equal to 1.\n",
    "for index, row in mod_p_dist.iterrows():\n",
    "    print(row.sum())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Comparison between softmax and sigmoid based Multiclass Logistic Regression"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "# Visualizing the results.\n",
    "plt.plot(loss_list, label='softmax-based')\n",
    "plt.plot(mod_loss_list, label='sigmoid-based')\n",
    "plt.xlabel(\"Epoch\")\n",
    "plt.ylabel(\"Training loss\")\n",
    "plt.legend()\n",
    "plt.savefig('graphs.png')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Time taken for softmax-based: 0.03468585014343262 sec\n",
      "Time taken for sigmoid-based: 1.087425947189331 sec\n"
     ]
    }
   ],
   "source": [
    "print(\"Time taken for softmax-based: {} sec\".format(softmax_time))\n",
    "print(\"Time taken for sigmoid-based: {} sec\".format(sigmoid_time))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Predicting test set.\n",
    "mod_predictions = mod_predict_labels(X_test.to_numpy(), W)\n",
    "\n",
    "# Decoding predicted values.\n",
    "mod_decoded_pred = []\n",
    "\n",
    "for pred in mod_predictions:\n",
    "    if (pred==0):\n",
    "        mod_decoded_pred.append('c_avellana')\n",
    "    if (pred==1):\n",
    "        mod_decoded_pred.append('c_americana')\n",
    "    if (pred==2):\n",
    "        mod_decoded_pred.append('c_cornuta')\n",
    "\n",
    "mod_compare = pd.DataFrame()\n",
    "mod_compare[\"predicted\"] = mod_decoded_pred\n",
    "mod_compare[\"true\"] = y_test.values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>predicted</th>\n",
       "      <th>true</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <td>0</td>\n",
       "      <td>c_americana</td>\n",
       "      <td>c_americana</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1</td>\n",
       "      <td>c_avellana</td>\n",
       "      <td>c_avellana</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2</td>\n",
       "      <td>c_cornuta</td>\n",
       "      <td>c_cornuta</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>3</td>\n",
       "      <td>c_cornuta</td>\n",
       "      <td>c_cornuta</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>4</td>\n",
       "      <td>c_avellana</td>\n",
       "      <td>c_avellana</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>5</td>\n",
       "      <td>c_cornuta</td>\n",
       "      <td>c_cornuta</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>6</td>\n",
       "      <td>c_avellana</td>\n",
       "      <td>c_avellana</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>7</td>\n",
       "      <td>c_cornuta</td>\n",
       "      <td>c_cornuta</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>8</td>\n",
       "      <td>c_cornuta</td>\n",
       "      <td>c_cornuta</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>9</td>\n",
       "      <td>c_avellana</td>\n",
       "      <td>c_avellana</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>10</td>\n",
       "      <td>c_cornuta</td>\n",
       "      <td>c_cornuta</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>11</td>\n",
       "      <td>c_avellana</td>\n",
       "      <td>c_avellana</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>12</td>\n",
       "      <td>c_cornuta</td>\n",
       "      <td>c_cornuta</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>13</td>\n",
       "      <td>c_cornuta</td>\n",
       "      <td>c_cornuta</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>14</td>\n",
       "      <td>c_cornuta</td>\n",
       "      <td>c_cornuta</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>15</td>\n",
       "      <td>c_cornuta</td>\n",
       "      <td>c_cornuta</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>16</td>\n",
       "      <td>c_avellana</td>\n",
       "      <td>c_avellana</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>17</td>\n",
       "      <td>c_avellana</td>\n",
       "      <td>c_avellana</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>18</td>\n",
       "      <td>c_americana</td>\n",
       "      <td>c_americana</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>19</td>\n",
       "      <td>c_avellana</td>\n",
       "      <td>c_avellana</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>20</td>\n",
       "      <td>c_avellana</td>\n",
       "      <td>c_avellana</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>21</td>\n",
       "      <td>c_avellana</td>\n",
       "      <td>c_avellana</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>22</td>\n",
       "      <td>c_americana</td>\n",
       "      <td>c_americana</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>23</td>\n",
       "      <td>c_americana</td>\n",
       "      <td>c_americana</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>24</td>\n",
       "      <td>c_cornuta</td>\n",
       "      <td>c_cornuta</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>25</td>\n",
       "      <td>c_cornuta</td>\n",
       "      <td>c_cornuta</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>26</td>\n",
       "      <td>c_cornuta</td>\n",
       "      <td>c_cornuta</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>27</td>\n",
       "      <td>c_avellana</td>\n",
       "      <td>c_avellana</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>28</td>\n",
       "      <td>c_avellana</td>\n",
       "      <td>c_avellana</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>29</td>\n",
       "      <td>c_cornuta</td>\n",
       "      <td>c_cornuta</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>30</td>\n",
       "      <td>c_avellana</td>\n",
       "      <td>c_avellana</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>31</td>\n",
       "      <td>c_americana</td>\n",
       "      <td>c_americana</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>32</td>\n",
       "      <td>c_cornuta</td>\n",
       "      <td>c_cornuta</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>33</td>\n",
       "      <td>c_cornuta</td>\n",
       "      <td>c_cornuta</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>34</td>\n",
       "      <td>c_americana</td>\n",
       "      <td>c_americana</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>35</td>\n",
       "      <td>c_americana</td>\n",
       "      <td>c_americana</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>36</td>\n",
       "      <td>c_cornuta</td>\n",
       "      <td>c_cornuta</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>37</td>\n",
       "      <td>c_avellana</td>\n",
       "      <td>c_americana</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>38</td>\n",
       "      <td>c_americana</td>\n",
       "      <td>c_americana</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>39</td>\n",
       "      <td>c_cornuta</td>\n",
       "      <td>c_cornuta</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>40</td>\n",
       "      <td>c_cornuta</td>\n",
       "      <td>c_cornuta</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>41</td>\n",
       "      <td>c_cornuta</td>\n",
       "      <td>c_cornuta</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>42</td>\n",
       "      <td>c_cornuta</td>\n",
       "      <td>c_cornuta</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>43</td>\n",
       "      <td>c_americana</td>\n",
       "      <td>c_americana</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>44</td>\n",
       "      <td>c_cornuta</td>\n",
       "      <td>c_cornuta</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>45</td>\n",
       "      <td>c_americana</td>\n",
       "      <td>c_americana</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>46</td>\n",
       "      <td>c_americana</td>\n",
       "      <td>c_americana</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>47</td>\n",
       "      <td>c_avellana</td>\n",
       "      <td>c_avellana</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>48</td>\n",
       "      <td>c_cornuta</td>\n",
       "      <td>c_cornuta</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>49</td>\n",
       "      <td>c_americana</td>\n",
       "      <td>c_americana</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>50</td>\n",
       "      <td>c_americana</td>\n",
       "      <td>c_americana</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>51</td>\n",
       "      <td>c_americana</td>\n",
       "      <td>c_americana</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>52</td>\n",
       "      <td>c_avellana</td>\n",
       "      <td>c_cornuta</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>53</td>\n",
       "      <td>c_americana</td>\n",
       "      <td>c_americana</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>54</td>\n",
       "      <td>c_avellana</td>\n",
       "      <td>c_avellana</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>55</td>\n",
       "      <td>c_avellana</td>\n",
       "      <td>c_cornuta</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>56</td>\n",
       "      <td>c_cornuta</td>\n",
       "      <td>c_cornuta</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>57</td>\n",
       "      <td>c_americana</td>\n",
       "      <td>c_americana</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>58</td>\n",
       "      <td>c_cornuta</td>\n",
       "      <td>c_cornuta</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>59</td>\n",
       "      <td>c_cornuta</td>\n",
       "      <td>c_cornuta</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "      predicted         true\n",
       "0   c_americana  c_americana\n",
       "1    c_avellana   c_avellana\n",
       "2     c_cornuta    c_cornuta\n",
       "3     c_cornuta    c_cornuta\n",
       "4    c_avellana   c_avellana\n",
       "5     c_cornuta    c_cornuta\n",
       "6    c_avellana   c_avellana\n",
       "7     c_cornuta    c_cornuta\n",
       "8     c_cornuta    c_cornuta\n",
       "9    c_avellana   c_avellana\n",
       "10    c_cornuta    c_cornuta\n",
       "11   c_avellana   c_avellana\n",
       "12    c_cornuta    c_cornuta\n",
       "13    c_cornuta    c_cornuta\n",
       "14    c_cornuta    c_cornuta\n",
       "15    c_cornuta    c_cornuta\n",
       "16   c_avellana   c_avellana\n",
       "17   c_avellana   c_avellana\n",
       "18  c_americana  c_americana\n",
       "19   c_avellana   c_avellana\n",
       "20   c_avellana   c_avellana\n",
       "21   c_avellana   c_avellana\n",
       "22  c_americana  c_americana\n",
       "23  c_americana  c_americana\n",
       "24    c_cornuta    c_cornuta\n",
       "25    c_cornuta    c_cornuta\n",
       "26    c_cornuta    c_cornuta\n",
       "27   c_avellana   c_avellana\n",
       "28   c_avellana   c_avellana\n",
       "29    c_cornuta    c_cornuta\n",
       "30   c_avellana   c_avellana\n",
       "31  c_americana  c_americana\n",
       "32    c_cornuta    c_cornuta\n",
       "33    c_cornuta    c_cornuta\n",
       "34  c_americana  c_americana\n",
       "35  c_americana  c_americana\n",
       "36    c_cornuta    c_cornuta\n",
       "37   c_avellana  c_americana\n",
       "38  c_americana  c_americana\n",
       "39    c_cornuta    c_cornuta\n",
       "40    c_cornuta    c_cornuta\n",
       "41    c_cornuta    c_cornuta\n",
       "42    c_cornuta    c_cornuta\n",
       "43  c_americana  c_americana\n",
       "44    c_cornuta    c_cornuta\n",
       "45  c_americana  c_americana\n",
       "46  c_americana  c_americana\n",
       "47   c_avellana   c_avellana\n",
       "48    c_cornuta    c_cornuta\n",
       "49  c_americana  c_americana\n",
       "50  c_americana  c_americana\n",
       "51  c_americana  c_americana\n",
       "52   c_avellana    c_cornuta\n",
       "53  c_americana  c_americana\n",
       "54   c_avellana   c_avellana\n",
       "55   c_avellana    c_cornuta\n",
       "56    c_cornuta    c_cornuta\n",
       "57  c_americana  c_americana\n",
       "58    c_cornuta    c_cornuta\n",
       "59    c_cornuta    c_cornuta"
      ]
     },
     "execution_count": 37,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "mod_compare"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Calculating the accuracy.\n",
    "mod_misclassified = 0\n",
    "for index, row in mod_compare.iterrows():\n",
    "    if row[\"predicted\"]==row[\"true\"]:\n",
    "        mod_misclassified += 1\n",
    "\n",
    "mod_rep_acc = mod_misclassified/mod_compare.shape[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Softmax-based MLR accuracy: 0.95\n",
      "Sigmoid-based MLR accuracy: 0.95\n"
     ]
    }
   ],
   "source": [
    "print(\"Softmax-based MLR accuracy:\", rep_acc)\n",
    "print(\"Sigmoid-based MLR accuracy:\", mod_rep_acc)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
